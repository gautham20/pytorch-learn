{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "path = data_path / 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (path / FILENAME).exists():\n",
    "    resp = requests.get(URL + FILENAME)\n",
    "    content = resp.content\n",
    "    (path / FILENAME).open('wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "!ls data/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((path / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8d8f96160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_train, y_train, x_valid, y_valid = map(torch.Tensor, (x_train, y_train, x_valid, y_valid))\n",
    "print(y_train.min())\n",
    "print(y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the `weights` and `bias` for the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784, 10) / (784**0.5)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@` is matrix multiply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([-2.2937, -2.4623, -2.7081, -2.1372, -2.6055, -2.4156, -2.2089, -2.8147,\n",
      "        -2.2886, -1.6353], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[:bs]\n",
    "preds = model(xb)\n",
    "print(preds.shape)\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((xb @ weights) + bias).exp().sum(1).log().unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((xb @ weights) + bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(preds, target):\n",
    "    return -preds[range(target.shape[0]), target.long()].mean()\n",
    "\n",
    "loss_func = negative_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(preds, target):\n",
    "    return -preds[0:target.shape[0], target.long()].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3487, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, target):\n",
    "    return (torch.argmax(preds, 1) == target.long()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "train_loss - 2.348705530166626 valid_loss - 2.0486879348754883 train_acc - 0.09375 valid_acc - 0.2736000120639801\n",
      "train_loss - 0.32130053639411926 valid_loss - 0.41520386934280396 train_acc - 0.890625 valid_acc - 0.8802000284194946\n",
      "train_loss - 0.30343303084373474 valid_loss - 0.3691319525241852 train_acc - 0.90625 valid_acc - 0.8927000164985657\n",
      "train_loss - 0.3882933259010315 valid_loss - 0.3505232036113739 train_acc - 0.921875 valid_acc - 0.8917999863624573\n",
      "train_loss - 0.23060409724712372 valid_loss - 0.3282702565193176 train_acc - 0.90625 valid_acc - 0.9045000076293945\n",
      "train_loss - 0.37336549162864685 valid_loss - 0.3155181407928467 train_acc - 0.890625 valid_acc - 0.9090999960899353\n",
      "train_loss - 0.2645023465156555 valid_loss - 0.3185669779777527 train_acc - 0.890625 valid_acc - 0.9107000231742859\n",
      "train_loss - 0.38044777512550354 valid_loss - 0.305110901594162 train_acc - 0.90625 valid_acc - 0.9120000004768372\n",
      "train_loss - 0.3970204293727875 valid_loss - 0.3109494149684906 train_acc - 0.9375 valid_acc - 0.9128000140190125\n",
      "epoch 1\n",
      "train_loss - 0.2720242142677307 valid_loss - 0.30481964349746704 train_acc - 0.9375 valid_acc - 0.9129999876022339\n",
      "train_loss - 0.26065441966056824 valid_loss - 0.316519170999527 train_acc - 0.921875 valid_acc - 0.910099983215332\n",
      "train_loss - 0.19617000222206116 valid_loss - 0.3008333742618561 train_acc - 0.90625 valid_acc - 0.9143999814987183\n",
      "train_loss - 0.34314507246017456 valid_loss - 0.3034951686859131 train_acc - 0.921875 valid_acc - 0.9097999930381775\n",
      "train_loss - 0.2081003487110138 valid_loss - 0.29870256781578064 train_acc - 0.9375 valid_acc - 0.9140999913215637\n",
      "train_loss - 0.350407212972641 valid_loss - 0.29597780108451843 train_acc - 0.90625 valid_acc - 0.9164000153541565\n",
      "train_loss - 0.2261524498462677 valid_loss - 0.29880988597869873 train_acc - 0.90625 valid_acc - 0.9169999957084656\n",
      "train_loss - 0.36298811435699463 valid_loss - 0.28996944427490234 train_acc - 0.890625 valid_acc - 0.9160000085830688\n",
      "train_loss - 0.3107762336730957 valid_loss - 0.2922055125236511 train_acc - 0.9375 valid_acc - 0.9172999858856201\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "lr = 0.5\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('epoch {0}'.format(e))\n",
    "    for i in range((n // bs) + 1):\n",
    "        xb = x_train[i*bs: (i + 1)*bs]\n",
    "        yb = y_train[i*bs: (i + 1)*bs]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            valid_preds = model(x_valid)\n",
    "            valid_loss = loss_func(valid_preds, y_valid)\n",
    "            train_acc = accuracy(preds, yb)\n",
    "            valid_acc = accuracy(valid_preds, y_valid)\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "        if i % 100 == 0:\n",
    "            print('train_loss - {0} valid_loss - {1} train_acc - {2} valid_acc - {3}'.format(loss, valid_loss, train_acc, valid_acc))\n",
    "    print('train_loss - {0} valid_loss - {1} train_acc - {2} valid_acc - {3}'.format(loss, valid_loss, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def torch_model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0826, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(torch_model(xb), yb.long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(torch_model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't using `nn` or `F` default functions, you I'll have to mention `nn.Parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class MnistLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistLogistic, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "    \n",
    "class MnistLogistic2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistLogistic2, self).__init__()\n",
    "        self.lin1 = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin1(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistLogistic2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3944, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss - 0.1640242338180542 valid_loss - 6.848804473876953 train_acc - 0.96875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.29727596044540405 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.1722470074892044 valid_loss - 6.848804473876953 train_acc - 0.953125 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.33796900510787964 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.20682793855667114 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.34444522857666016 valid_loss - 6.848804473876953 train_acc - 0.859375 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.18046662211418152 valid_loss - 6.848804473876953 train_acc - 0.953125 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.374267041683197 valid_loss - 6.848804473876953 train_acc - 0.859375 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.16367851197719574 valid_loss - 6.848804473876953 train_acc - 0.96875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.29724442958831787 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.17206856608390808 valid_loss - 6.848804473876953 train_acc - 0.953125 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.3371374309062958 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.2066117525100708 valid_loss - 6.848804473876953 train_acc - 0.921875 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.3439042866230011 valid_loss - 6.848804473876953 train_acc - 0.859375 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.1804623007774353 valid_loss - 6.848804473876953 train_acc - 0.953125 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.374366819858551 valid_loss - 6.848804473876953 train_acc - 0.859375 valid_acc - 0.9172999858856201\n",
      "train_loss - 0.19518420100212097 valid_loss - 6.848804473876953 train_acc - 0.9375 valid_acc - 0.9172999858856201\n"
     ]
    }
   ],
   "source": [
    "def fit(epochs=2, lr=0.5, bs=64, use_optim=True):\n",
    "    opt = None\n",
    "    if use_optim:\n",
    "        opt = optim.SGD(model.parameters(), lr)\n",
    "    for e in range(epochs):\n",
    "        for i in range(n // bs):\n",
    "            xb = x_train[i * bs: (i + 1) * bs]\n",
    "            yb = y_train[i * bs: (i + 1) * bs]\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(model(xb), yb.long())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                if not opt:\n",
    "                    for p in model.parameters():\n",
    "                        p -= p.grad * lr\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "                valid_preds = model(x_valid)\n",
    "                valid_loss = loss_func(valid_preds, y_valid.long())\n",
    "                train_acc = accuracy(preds, yb)\n",
    "                valid_acc = accuracy(valid_preds, y_valid)\n",
    "                if i % 100 == 0:\n",
    "                    print('train_loss - {0} valid_loss - {1} train_acc - {2} valid_acc - {3}'.format(loss, valid_loss, train_acc, valid_acc))\n",
    "    print('train_loss - {0} valid_loss - {1} train_acc - {2} valid_acc - {3}'.format(loss, valid_loss, train_acc, valid_acc))\n",
    "\n",
    "\n",
    "fit(lr=0.001)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss - 0.357680082321167 valid_loss - 0.2944393754005432\n",
      "train_loss - 0.2946665585041046 valid_loss - 0.28494060039520264\n"
     ]
    }
   ],
   "source": [
    "def fit2(epochs=2, lr=0.5, bs=64, use_optim=True):\n",
    "    opt = None\n",
    "    if use_optim:\n",
    "        opt = optim.SGD(model.parameters(), lr)\n",
    "    for e in range(epochs):\n",
    "        for i, (xb, yb) in enumerate(train_dl):\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(model(xb), yb.long())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                if not opt:\n",
    "                    for p in model.parameters():\n",
    "                        p -= p.grad * lr\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "        valid_loss = np.sum([loss_func(model(xbv), ybv.long()) for xbv, ybv in valid_dl])/(len(valid_dl))\n",
    "        print('train_loss - {0} valid_loss - {1}'.format(loss, valid_loss))\n",
    "\n",
    "\n",
    "fit2(lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb.long())\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit3(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*(loss_batch(model, loss_func, xbv, ybv) for xbv, ybv in valid_dl))\n",
    "            valid_loss =  np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            print('epoch-{0} validation loss {1}'.format(e, valid_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 0.27994970932006835\n",
      "epoch-1 validation loss 0.2802725152015686\n",
      "epoch-2 validation loss 0.2806073511600494\n",
      "epoch-3 validation loss 0.28094887571334837\n",
      "epoch-4 validation loss 0.28129335680007933\n",
      "epoch-5 validation loss 0.2816379490852356\n",
      "epoch-6 validation loss 0.28198056259155274\n",
      "epoch-7 validation loss 0.28231973133087157\n",
      "epoch-8 validation loss 0.2826543620109558\n",
      "epoch-9 validation loss 0.2829836194038391\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), 0.5)\n",
    "fit3(10, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def get_model(lr):\n",
    "    model = MnistLogistic2()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 0.33959490728378294\n",
      "epoch-1 validation loss 0.3086001440525055\n",
      "epoch-2 validation loss 0.29585678944587707\n",
      "epoch-3 validation loss 0.29204615168571474\n",
      "epoch-4 validation loss 0.2835223401069641\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model(lr)\n",
    "fit3(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 0.33756188468933107\n",
      "epoch-1 validation loss 0.21521428208351134\n",
      "epoch-2 validation loss 0.18969518237113953\n",
      "epoch-3 validation loss 0.1669872248649597\n",
      "epoch-4 validation loss 0.1542125696182251\n"
     ]
    }
   ],
   "source": [
    "model = MnistConv()\n",
    "opt = optim.SGD(model.parameters(), 0.3, momentum=0.9)\n",
    "\n",
    "fit3(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 0.45498179564476016\n",
      "epoch-1 validation loss 0.2638790647983551\n",
      "epoch-2 validation loss 0.23730865325927733\n",
      "epoch-3 validation loss 0.17411421422958373\n",
      "epoch-4 validation loss 0.1851066417694092\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(-1, x.size(1)))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), momentum=0.9, lr=0.05)\n",
    "fit3(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 0.1530505248069763\n",
      "epoch-1 validation loss 0.14628950057029724\n",
      "epoch-2 validation loss 0.1441276515007019\n",
      "epoch-3 validation loss 0.14568585429191588\n",
      "epoch-4 validation loss 0.14490982742309572\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), momentum=0.9, lr=0.01)\n",
    "fit3(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9612)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_valid), y_valid.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(xb, yb):\n",
    "    return xb.view(-1, 1, 28, 28).to(dev), yb.to(dev)\n",
    "\n",
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for xb, yb in batches:\n",
    "            yield(self.func(xb, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  (7): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "model.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-0 validation loss 1.2577770030975342\n",
      "epoch-1 validation loss 0.654534202337265\n",
      "epoch-2 validation loss 0.4766000895500183\n",
      "epoch-3 validation loss 0.3947130201339722\n",
      "epoch-4 validation loss 0.3195445415496826\n",
      "epoch-5 validation loss 0.30788535861968996\n",
      "epoch-6 validation loss 0.2760514275550842\n",
      "epoch-7 validation loss 0.29421159467697144\n",
      "epoch-8 validation loss 0.2523993155479431\n",
      "epoch-9 validation loss 0.22187280511856078\n",
      "epoch-10 validation loss 0.20684781351089476\n",
      "epoch-11 validation loss 0.20437439575195313\n",
      "epoch-12 validation loss 0.19186659498214723\n",
      "epoch-13 validation loss 0.20524548101425172\n",
      "epoch-14 validation loss 0.2066484146118164\n",
      "epoch-15 validation loss 0.19135530433654785\n",
      "epoch-16 validation loss 0.1951168583869934\n",
      "epoch-17 validation loss 0.2017732940673828\n",
      "epoch-18 validation loss 0.18288356914520262\n",
      "epoch-19 validation loss 0.19482524271011353\n",
      "epoch-20 validation loss 0.1736426918029785\n",
      "epoch-21 validation loss 0.1928091586112976\n",
      "epoch-22 validation loss 0.17435116424560546\n",
      "epoch-23 validation loss 0.17404209699630738\n",
      "epoch-24 validation loss 0.17107018013000488\n",
      "epoch-25 validation loss 0.19200367603302002\n",
      "epoch-26 validation loss 0.16378958539962768\n",
      "epoch-27 validation loss 0.15686916971206666\n",
      "epoch-28 validation loss 0.17926291723251342\n",
      "epoch-29 validation loss 0.167744718170166\n",
      "epoch-30 validation loss 0.15577890453338622\n",
      "epoch-31 validation loss 0.20835046339035035\n",
      "epoch-32 validation loss 0.15447113847732544\n",
      "epoch-33 validation loss 0.15021207447052\n",
      "epoch-34 validation loss 0.1566745970726013\n",
      "epoch-35 validation loss 0.17352079572677612\n",
      "epoch-36 validation loss 0.15360490579605102\n",
      "epoch-37 validation loss 0.1647948835372925\n",
      "epoch-38 validation loss 0.1442491951942444\n",
      "epoch-39 validation loss 0.14960820770263672\n",
      "epoch-40 validation loss 0.14525529375076293\n",
      "epoch-41 validation loss 0.15516737909317016\n",
      "epoch-42 validation loss 0.14232773761749268\n",
      "epoch-43 validation loss 0.14302343187332153\n",
      "epoch-44 validation loss 0.14523550786972045\n",
      "epoch-45 validation loss 0.13621472578048707\n",
      "epoch-46 validation loss 0.13665307788848877\n",
      "epoch-47 validation loss 0.13446856966018678\n",
      "epoch-48 validation loss 0.15127236833572388\n",
      "epoch-49 validation loss 0.1663764281272888\n",
      "epoch-50 validation loss 0.13503173503875732\n",
      "epoch-51 validation loss 0.15507103137969971\n",
      "epoch-52 validation loss 0.14566686363220216\n",
      "epoch-53 validation loss 0.14228879919052123\n",
      "epoch-54 validation loss 0.15390530672073363\n",
      "epoch-55 validation loss 0.1405298899650574\n",
      "epoch-56 validation loss 0.135159765625\n",
      "epoch-57 validation loss 0.1317846561431885\n",
      "epoch-58 validation loss 0.1274750612258911\n",
      "epoch-59 validation loss 0.12756743021011352\n",
      "epoch-60 validation loss 0.1331629590034485\n",
      "epoch-61 validation loss 0.1495718710899353\n",
      "epoch-62 validation loss 0.12619897832870483\n",
      "epoch-63 validation loss 0.12730369215011597\n",
      "epoch-64 validation loss 0.1261827886581421\n",
      "epoch-65 validation loss 0.12858368892669678\n",
      "epoch-66 validation loss 0.12470947923660278\n",
      "epoch-67 validation loss 0.13180683364868165\n",
      "epoch-68 validation loss 0.1326748028755188\n",
      "epoch-69 validation loss 0.12668358058929444\n",
      "epoch-70 validation loss 0.13463326673507692\n",
      "epoch-71 validation loss 0.12319372644424438\n",
      "epoch-72 validation loss 0.1338562731742859\n",
      "epoch-73 validation loss 0.1223500626564026\n",
      "epoch-74 validation loss 0.12461156330108643\n",
      "epoch-75 validation loss 0.11853289623260498\n",
      "epoch-76 validation loss 0.1231174572944641\n",
      "epoch-77 validation loss 0.12385884380340575\n",
      "epoch-78 validation loss 0.1209412486076355\n",
      "epoch-79 validation loss 0.11980782880783081\n",
      "epoch-80 validation loss 0.1274437418937683\n",
      "epoch-81 validation loss 0.13076568822860718\n",
      "epoch-82 validation loss 0.1191203049659729\n",
      "epoch-83 validation loss 0.12345316171646119\n",
      "epoch-84 validation loss 0.1165073727607727\n",
      "epoch-85 validation loss 0.11650983467102051\n",
      "epoch-86 validation loss 0.12780948343276977\n",
      "epoch-87 validation loss 0.11397910614013672\n",
      "epoch-88 validation loss 0.11861690845489502\n",
      "epoch-89 validation loss 0.11851550302505494\n",
      "epoch-90 validation loss 0.12378312187194825\n",
      "epoch-91 validation loss 0.11145643472671508\n",
      "epoch-92 validation loss 0.1136980257987976\n",
      "epoch-93 validation loss 0.11515540037155152\n",
      "epoch-94 validation loss 0.11083812370300293\n",
      "epoch-95 validation loss 0.11502846460342407\n",
      "epoch-96 validation loss 0.12081747188568115\n",
      "epoch-97 validation loss 0.11741378927230835\n",
      "epoch-98 validation loss 0.10881231517791748\n",
      "epoch-99 validation loss 0.10964945287704468\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "fit3(100, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697389240506329"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([accuracy(model(xbv), ybv).item() for xbv, ybv in valid_dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
